[{"content":"This website is an experiment of sorts. Over the last year or so, I have been playing around with logseq as a note taking tool. Looking for a way to better share my notes, I have found the Schrodinger plugin by Aryan Sawhney.\nThis is what happened next.\n","permalink":"https://vonhartz.github.io/pages/welcome/","summary":"\u003cp\u003eThis website is an experiment of sorts.\nOver the last year or so, I have been playing around with \u003ca href=\"https://docs.logseq.com\"\u003elogseq\u003c/a\u003e as a note taking tool.\nLooking for a way to better share my notes, I have found the \u003ca href=\"https://github.com/sawhney17/logseq-schrodinger\"\u003eSchrodinger plugin\u003c/a\u003e by Aryan Sawhney.\u003c/p\u003e\n\u003cp\u003eThis is what happened next.\u003c/p\u003e","title":"Welcome"},{"content":"This is a test/examle post.\nHow to\n\u0026gt;\u0026gt;\u0026gt; import torch \u0026gt;\u0026gt;\u0026gt; import matplotlib.pyplot as plt \u0026gt;\u0026gt;\u0026gt; def plot(pr, sm, po, channel): ... for i, d in enumerate([pr[channel], sm[channel], po[channel], (sm*pr)[channel]]): ... plt.subplot(1, 4, i+1) ... plt.imshow(d) ... plt.show() \u0026gt;\u0026gt;\u0026gt; pr = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_prior/80.dat\u0026#39;) \u0026gt;\u0026gt;\u0026gt; po = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_post/80.dat\u0026#39;) \u0026gt;\u0026gt;\u0026gt; sm = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_sm/80.dat\u0026#39;) Results for sigma=2\nChannels: 10, 2, 14\nWhile on 2 it resolves the multimodality, on channel 10 it does not manage to do it. On Channel 14 it further manages to sharpen the cluster, but sometimes it also fails to do that.\nI think the noise component in the motion model is too strong.\nIf you look at successive frames of the same channel, sometimes multi-modalities would spring up later in the trajectory.\nProblem was that the softmax was stored as the prior for the next step, not the posterior.\nFixing it, resolves that and does in fact improve kp localization.\nThe numerical effect is not very large though.\nCould be that collapse of multi-modalities is not correct, but stable across a trajectory. In that case should still see improvement in policy learning.\nBut, I also observed some keypoints still moving. Could be that this only happens when the kp is moving outside too much.\nCould be that smaller sigma helps.\n","permalink":"https://vonhartz.github.io/pages/discrete-filter-debug/","summary":"\u003cp\u003eThis is a test/examle post.\u003c/p\u003e\n\u003cp\u003eHow to\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e\u0026gt;\u0026gt;\u0026gt; import torch\n\u0026gt;\u0026gt;\u0026gt; import matplotlib.pyplot as plt\n\n\u0026gt;\u0026gt;\u0026gt; def plot(pr, sm, po, channel):\n...     for i, d in enumerate([pr[channel], sm[channel], po[channel], (sm*pr)[channel]]):\n...             plt.subplot(1, 4, i+1)\n...             plt.imshow(d)\n...     plt.show()\n\n\u0026gt;\u0026gt;\u0026gt; pr = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_prior/80.dat\u0026#39;)\n\u0026gt;\u0026gt;\u0026gt; po = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_post/80.dat\u0026#39;)\n\u0026gt;\u0026gt;\u0026gt; sm = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_sm/80.dat\u0026#39;)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eResults for sigma=2\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eChannels: 10, 2, 14\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhile on 2 it resolves the multimodality, on channel 10 it does not manage to do it. On Channel 14 it further manages to sharpen the cluster, but sometimes it also fails to do that.\u003c/p\u003e","title":"Discrete Filter Debug"},{"content":" Currently PhD student at the neurorobotics and robot learning labs, University of Freiburg.\nSpecialized in Reinforcement learning, algorithms, cognitive computation.\nResearch interests Sequential decision problems, policy learning, embodied cognition.\nEducation 2020 - 2022 MSc Computer Science, University of Freiburg (Germany) Minor: Neuroscience\n2019 - 2020 Erasmus stay at ESIEE Paris (France)\n2015 - 2019 BSc Computer Science, University of Freiburg (Germany) Minor: Cognitive Science\nPublications Journals Forthcoming von Hartz, Jan Ole, and Chisari, Eugenio and Welschehold, Tim and Valada, Abhinav. ”The Treachery of Images: Re-Imagining Representation Learning as Scene Representation using Bayes Filters”.\n2018 Dames, Hannah and von Hartz, Jan Ole and Kantz, Mario and Riesterer, Nicolas and Ragni, Marco (2018), “Multinomial Processing Models for Syllogistic Reasoning: A Comparison”, Paper presented at the 40th annual cognitive science society meeting http://mindmodeling.org/cogsci2018/papers/0305/0305.pdf\nWorkshops 2022 von Hartz, Jan Ole, and Chisari, Eugenio and Welschehold, Tim Valada, Abhinav. ”Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation.” In ICRA 2022 Workshop: Reinforcement Learning for Contact-Rich Manipulation.\nCommunity involvement Reviewer Duties 2022 NeurIPS Datasets and Benchmarks\n","permalink":"https://vonhartz.github.io/pages/cv/","summary":"\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003ch2 id=\"currently\"\u003eCurrently\u003c/h2\u003e\n\u003cp\u003ePhD student at the neurorobotics and robot learning labs, University of Freiburg.\u003c/p\u003e\n\u003ch3 id=\"specialized-in\"\u003eSpecialized in\u003c/h3\u003e\n\u003cp\u003eReinforcement learning, algorithms, cognitive computation.\u003c/p\u003e\n\u003ch3 id=\"research-interests\"\u003eResearch interests\u003c/h3\u003e\n\u003cp\u003eSequential decision problems, policy learning, embodied cognition.\u003c/p\u003e\n\u003ch2 id=\"education\"\u003eEducation\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003e2020 - 2022\u003c/code\u003e\n\u003cstrong\u003eMSc\u003c/strong\u003e Computer Science, University of Freiburg (Germany)\nMinor: Neuroscience\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e2019 - 2020\u003c/code\u003e\n\u003cstrong\u003eErasmus\u003c/strong\u003e stay at ESIEE Paris (France)\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e2015 - 2019\u003c/code\u003e\n\u003cstrong\u003eBSc\u003c/strong\u003e Computer Science, University of Freiburg (Germany)\nMinor: Cognitive Science\u003c/p\u003e","title":"Curriculum vitae"}]