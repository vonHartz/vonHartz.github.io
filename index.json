[{"content":"This website is an experiment of sorts. Over the last year or so, I have been playing around with logseq as a note taking tool. Looking for a way to better share my notes, I have found the Schrodinger plugin by Aryan Sawhney.\nThis is what happened next.\n","permalink":"https://vonhartz.info/pages/welcome/","summary":"This website is an experiment of sorts. Over the last year or so, I have been playing around with logseq as a note taking tool. Looking for a way to better share my notes, I have found the Schrodinger plugin by Aryan Sawhney.\nThis is what happened next.","title":"Welcome"},{"content":"This is a test/examle post.\nHow to\n\u0026gt;\u0026gt;\u0026gt; import torch \u0026gt;\u0026gt;\u0026gt; import matplotlib.pyplot as plt \u0026gt;\u0026gt;\u0026gt; def plot(pr, sm, po, channel): ... for i, d in enumerate([pr[channel], sm[channel], po[channel], (sm*pr)[channel]]): ... plt.subplot(1, 4, i+1) ... plt.imshow(d) ... plt.show() \u0026gt;\u0026gt;\u0026gt; pr = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_prior/80.dat\u0026#39;) \u0026gt;\u0026gt;\u0026gt; po = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_post/80.dat\u0026#39;) \u0026gt;\u0026gt;\u0026gt; sm = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_sm/80.dat\u0026#39;) Results for sigma=2\nChannels: 10, 2, 14\nWhile on 2 it resolves the multimodality, on channel 10 it does not manage to do it. On Channel 14 it further manages to sharpen the cluster, but sometimes it also fails to do that.\nI think the noise component in the motion model is too strong.\nIf you look at successive frames of the same channel, sometimes multi-modalities would spring up later in the trajectory.\nProblem was that the softmax was stored as the prior for the next step, not the posterior.\nFixing it, resolves that and does in fact improve kp localization.\nThe numerical effect is not very large though.\nCould be that collapse of multi-modalities is not correct, but stable across a trajectory. In that case should still see improvement in policy learning.\nBut, I also observed some keypoints still moving. Could be that this only happens when the kp is moving outside too much.\nCould be that smaller sigma helps.\n","permalink":"https://vonhartz.info/pages/discrete-filter-debug/","summary":"This is a test/examle post.\nHow to\n\u0026gt;\u0026gt;\u0026gt; import torch \u0026gt;\u0026gt;\u0026gt; import matplotlib.pyplot as plt \u0026gt;\u0026gt;\u0026gt; def plot(pr, sm, po, channel): ... for i, d in enumerate([pr[channel], sm[channel], po[channel], (sm*pr)[channel]]): ... plt.subplot(1, 4, i+1) ... plt.imshow(d) ... plt.show() \u0026gt;\u0026gt;\u0026gt; pr = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_prior/80.dat\u0026#39;) \u0026gt;\u0026gt;\u0026gt; po = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_post/80.dat\u0026#39;) \u0026gt;\u0026gt;\u0026gt; sm = torch.load(\u0026#39;/export/hartzj/TakeLidOffSaucepan/demos_gt/encodings/dcm_10_keypoints_encoder-resnet101bs128split9010/new_ego/2022_05_03-14_34_44/cam_w_sm/80.dat\u0026#39;) Results for sigma=2\nChannels: 10, 2, 14\nWhile on 2 it resolves the multimodality, on channel 10 it does not manage to do it.","title":"Discrete Filter Debug"}]